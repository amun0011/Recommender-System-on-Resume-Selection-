{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender System: Ranking resumes with respect to the given job advertisements\n",
    "#### Name: Arunava Munshi\n",
    "#### Date:03-Sep-2018\n",
    "#### Environment: Python 3 and Jupyter notebook\n",
    "#### Libraries used: \n",
    "* String (for String Processng)\n",
    "* nltk - natural language toolkit (tokenizer, lemmatizer, stopwords, collocations and probabilities)\n",
    "* re (for regular expression, included in Anaconda Python 2.7) \n",
    "* itertools (for iterations)\n",
    "* collections (for Collocations)\n",
    "\n",
    "# Introduction\n",
    "The purpose of this project is to demonstrate how wrangled data from different sources can be used to help the organizations to make informed decisions. In this project, the system recommends the top 10 resumes that are the best fit for the first 500 job advertisements in 'raw_data.dat' their “required qualifications” section. The resume files are given in 'resume_dataset.txt', from which resumes are picked and recommended for suitable job profiles.\n",
    "\n",
    "Output files: Recommender_System.ipynb and Recommended_Resume.txt which contains the recommended resumes for the first 500 job advertisements. The txt file contains 500 lines and each line of the txt file has the following format: Job_advertisment_id: first_ranked_resume_id, second_ranked_resume_id, …., tenth_ranked_resume_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Importing Packages\n",
    "All the required packages are imported before being started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from itertools import chain\n",
    "import itertools\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Creating Separate Lists of Job Ids and Job Responsibilities after Stopword Removal and Sremming\n",
    "The following steps need to be performed in order to do the above \n",
    "## Reading the given raw_data.dat file and Taking 1st 1000 Records\n",
    "The below code reads the content of **'raw_data.dat'** file. Because the file is too big, a partial reading would be the best for the performance improvement. In the given file, it can be seen that the data regarding the job postings are given and each and every job posting is separated by a '------------------------------'. So this string became separator between two jobs and it is used in as a separator in reading in the below code. the variable **'readfile'** contains a list of individual job postings. There are also some exception handling done for the file processing. If the file is not found, then the FileNotFoundError is thrown, when there is permission error, PermissionError exception is raised etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:                    #Reading file raw_data.dat and splitting on the below eparator\n",
    "    readfile = open('raw_data.dat').read().split('------------------------------')\n",
    "    readfile.remove(readfile[len(readfile) - 1]) #Removing the last item from the list as it is null\n",
    "    all_job_postings_list = readfile[:1000] #Taking out 1st 1000 Occurrences\n",
    "except FileNotFoundError:\n",
    "    raise\n",
    "except PermissionError:\n",
    "    raise\n",
    "except OSError:\n",
    "    raise\n",
    "except:\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Stop Word File: stopwords_en.txt\n",
    "This file is read with a split on '\\n' and the stopwords are stored into a list named **'stop_word_list'** which is kept for later use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_file = open('stopwords_en.txt', 'r') #Opening and read each stopwords_en.txt file in read mode\n",
    "stop_word_list = stop_word_file.read().split('\\n') #Splitting items on '\\n'\n",
    "stop_word_file.close() #Closing the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking Out 1st 500 Job Responsibilities\n",
    "The below portion of code extracts the 1st 500 Job reponsibilities and put them into the dictionary all_job_postings_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_counter = 1\n",
    "maximum_items_no = len(all_job_postings_list)  #Length of readfile\n",
    "all_job_postings_dict = {}\n",
    "for items in all_job_postings_list:\n",
    "    \n",
    "    temp_item = '' \n",
    "    if current_counter <= 500:  #Cheking in current counter is <= Maximum items in the list or not\n",
    "        #The below Regular Expressions find the specific patterns and substitute them with a constant one\n",
    "        replaced_str = re.sub('(ID:)', '^_id^', items)\n",
    "        replaced_str = re.sub('(DATE_START:|DATES:|start_date:|START DATE:|START_DA:)', '^start_date^', replaced_str)\n",
    "        replaced_str = re.sub('(job_desc:|_description:|JOB_DESC:|DESCRIPTION:|JOB DESCRIPTION:)', '^job_descriptions^', replaced_str)\n",
    "        replaced_str = re.sub('(RESPONSIBILITY:|JOB RESPONSIBILITIES:|responsibilities:|JOB_RESPS:|RESP:)', '^job_responsibilities^', replaced_str)\n",
    "        replaced_str = re.sub('(title:|JOB TITLE:|JOB_T:|TITLES:|_TTL:)', '^title^', replaced_str)\n",
    "        replaced_str = re.sub('(ABOUT COMPANY:|about_company:|COMPANYS_INFO:|ABOUT:|_info:)', '^about_company^', replaced_str)\n",
    "        replaced_str = re.sub('(QUALIFS:|QUALIFICATION:|REQUIRED QUALIFICATIONS:|REQ_QUALS:|qualifications:)', '^required_qualifications^', replaced_str)\n",
    "        replaced_str = re.sub('(APPLICATION_DL:|APPLICATION_DEADL:|DEAD_LINE:|DEADLINES:|deadline:)', '^application_deadline^', replaced_str)\n",
    "        replaced_str = re.sub('(JOB_PROCS:|PROCEDURES:|JOB_PROC:|PROCEDURE:|procedures:)', '^application_procedure^', replaced_str)\n",
    "        replaced_str = re.sub('(LOCATION:|JOB_LOC:|LOCATIONS:|_LOC:|_LOCS:)', '^location^', replaced_str)\n",
    "        replaced_str = re.sub('(JOB_SAL:|REMUNERATION:|SALARY:|remuneration:|salary:)', '^salary^', replaced_str)\n",
    "        #Avoiding garbage records by substituting them with null character\n",
    "        replaced_str = re.sub('(REMUNERATION\\/|START DATE\\/|ABOUT PROGRAM\\/|OPEN TO\\/)', '', replaced_str)\n",
    "        \n",
    "        replaced_list = replaced_str.split('^')  #Splitting the substituted string on '^'\n",
    "        replaced_dict = {}\n",
    "        punctuation = '!\"#$%&\\'*+,-./:;<=>?@\\\\^_`|~ \\n' #Storing the list of punctuation\n",
    "        temp_qualification = ''\n",
    "        temp_id = ''\n",
    "        if 'required_qualifications' in replaced_list:\n",
    "            for items in replaced_list:        #Iterating each items in replaced_list\n",
    "                if items == '_id':\n",
    "                    item_index = replaced_list.index(items)\n",
    "                    temp_id = replaced_list[item_index + 1].strip(punctuation).replace('\\n',' ')\n",
    "                                    #Populating dictionary items after replacing '\\n' with ' and python striping\n",
    "                if items == 'required_qualifications':\n",
    "                    item_index = replaced_list.index(items)\n",
    "                    temp_qualification = replaced_list[item_index + 1].strip(punctuation).replace('\\n',' ')\n",
    "                                    #Populating dictionary items after replacing '\\n' with ' and python striping \n",
    "\n",
    "                all_job_postings_dict[temp_id] = temp_qualification\n",
    "\n",
    "            current_counter +=1\n",
    "all_job_postings_dict.pop('', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing the Stopwords and the Tokens with length lss than 3\n",
    "The Below code removes the Stopwords and the Tokens with length less than 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english') #Get the English Stopwords\n",
    "for keys, values in all_job_postings_dict.items(): #Traverse for all job responsibilities\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+(?:[-.]\\w+)?\") #Word tokenize using Regular Expression\n",
    "    unigram_tokens = tokenizer.tokenize(values) #Creating Unigram tokens\n",
    "    #Joining back the tokens into strings after stop word and token less than length 3 removal\n",
    "    all_job_postings_dict[keys] = ' '.join(set([ items for items in unigram_tokens if items.lower() not in stopwords_list or len(items) >= 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tokenized Dictionary and All Vocab List\n",
    "In this step two different objects are formed:\n",
    "> - all_all_job_postings_dict_tokenized dictinary that consists of each document id as key and list of unigram tokens for each document as value. This dictionary is created by invoking a method named tokenizePatent() which actually generates unigram word tokens from the string of each job responsibility content.\n",
    "> - From all_job_postings_dict_tokenized dictionary, the list of all vocabularies is created by chain.from_iterable() method which takes the list of lists as input and returns a single list of all items across this list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizePatent(pid):\n",
    "    \"\"\"\n",
    "        the tokenization function is used to tokenize each patent.\n",
    "        The one argument is patent_id.\n",
    "        First, normalize the case.\n",
    "        Then, use the regular expression tokenizer to tokenize the patent with the specified id\n",
    "    \"\"\"\n",
    "    each_dict_item = all_job_postings_dict[pid]\n",
    "    each_dict_item_tokenized = tokenizer.tokenize(each_dict_item)\n",
    "    return (pid, each_dict_item_tokenized) # return a tupel of patent_id and a list of tokens\n",
    "\n",
    "#Getting the tokenized version of the dictionary where each word token is separated\n",
    "all_job_postings_dict_tokenized = dict(tokenizePatent(pid) for pid in all_job_postings_dict.keys())\n",
    "#Getting the whole word lists\n",
    "all_job_postings_words = list(chain.from_iterable(all_job_postings_dict_tokenized.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Top 200 Bigrams\n",
    "A bigram is an asociation of two meaningful words. The below code creates top 200 meaningful bigrams in form of a list of tuples with the help of the full vacab list all_job_postings_words, created in the previous step. This program also set frequency filter at the level of 10 and also filters out those bigrams with length less than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SQL', 'Experience'),\n",
       " ('fields', 'University'),\n",
       " ('preferably', 'Economics'),\n",
       " ('team', 'within'),\n",
       " ('information', 'Ability'),\n",
       " ('Office', 'Analytical'),\n",
       " ('Office', 'Proficiency'),\n",
       " ('equivalent', 'degree'),\n",
       " ('service', 'Excellent'),\n",
       " ('Higher', 'Armenian'),\n",
       " ('professional', 'least'),\n",
       " ('written', 'both'),\n",
       " ('Excellent', 'desirable'),\n",
       " ('Java', 'knowledge'),\n",
       " ('Ability', 'independently'),\n",
       " ('data', 'experience'),\n",
       " ('the', 'public'),\n",
       " ('both', 'communication'),\n",
       " ('related', 'field'),\n",
       " ('experience', 'International'),\n",
       " ('NET', 'and'),\n",
       " ('working', 'Good'),\n",
       " ('solving', 'work'),\n",
       " ('years', 'Proven'),\n",
       " ('experience', 'Finance'),\n",
       " ('Bachelor', 'degree'),\n",
       " ('international', 'with'),\n",
       " ('years', 'Bank'),\n",
       " ('work', 'office'),\n",
       " ('languages', 'financial'),\n",
       " ('Ability', 'attention'),\n",
       " ('management', 'working'),\n",
       " ('personality', 'knowledge'),\n",
       " ('Team', 'Ability'),\n",
       " ('Good', 'education'),\n",
       " ('plus', 'years'),\n",
       " ('preferable', 'Knowledge'),\n",
       " ('thinking', 'Good'),\n",
       " ('and', 'deadlines'),\n",
       " ('Excel', 'Excellent'),\n",
       " ('Experience', 'language'),\n",
       " ('within', 'communication'),\n",
       " ('language', 'preferred'),\n",
       " ('Russian', 'computer'),\n",
       " ('responsibility', 'Computer'),\n",
       " ('with', 'Armenia'),\n",
       " ('years', 'other'),\n",
       " ('Proven', 'the'),\n",
       " ('communication', 'multiple'),\n",
       " ('other', 'the'),\n",
       " ('Armenian', 'banking'),\n",
       " ('year', 'languages'),\n",
       " ('degree', 'spoken'),\n",
       " ('Office', 'personality'),\n",
       " ('language', 'work'),\n",
       " ('communication', 'plus'),\n",
       " ('Word', 'Strong'),\n",
       " ('software', 'University'),\n",
       " ('multiple', 'years'),\n",
       " ('least', 'good'),\n",
       " ('desirable', 'Russian'),\n",
       " ('team', 'communication'),\n",
       " ('Business', 'work'),\n",
       " ('responsibility', 'Office'),\n",
       " ('deadlines', 'experience'),\n",
       " ('years', 'databases'),\n",
       " ('good', 'Excellent'),\n",
       " ('degree', 'Armenian'),\n",
       " ('Computer', 'Strong'),\n",
       " ('experience', 'Administration'),\n",
       " ('University', 'relevant'),\n",
       " ('experience', 'applications'),\n",
       " ('languages', 'sense'),\n",
       " ('Excellent', 'international'),\n",
       " ('languages', 'oral'),\n",
       " ('work', 'strong'),\n",
       " ('Knowledge', 'High'),\n",
       " ('with', 'understanding'),\n",
       " ('Russian', 'finance'),\n",
       " ('sense', 'language'),\n",
       " ('the', 'projects'),\n",
       " ('Good', 'level'),\n",
       " ('Fluency', 'the'),\n",
       " ('knowledge', 'programs'),\n",
       " ('work', 'thinking'),\n",
       " ('Proficiency', 'knowledge'),\n",
       " ('Armenian', 'tasks'),\n",
       " ('Excellent', 'Russian'),\n",
       " ('years', 'Fluency'),\n",
       " ('Knowledge', 'Work'),\n",
       " ('preferred', 'work'),\n",
       " ('Ability', 'literacy'),\n",
       " ('spoken', 'Armenian'),\n",
       " ('systems', 'Knowledge'),\n",
       " ('new', 'knowledge'),\n",
       " ('technical', 'least'),\n",
       " ('experience', 'Internet'),\n",
       " ('Advanced', 'least'),\n",
       " ('leadership', 'and'),\n",
       " ('Office', 'knowledge'),\n",
       " ('Armenian', 'writing'),\n",
       " ('knowledge', 'skills'),\n",
       " ('sense', 'work'),\n",
       " ('working', 'education'),\n",
       " ('work', 'management'),\n",
       " ('programs', 'and'),\n",
       " ('degree', 'writing'),\n",
       " ('least', 'Excel'),\n",
       " ('English', 'and'),\n",
       " ('knowledge', 'English'),\n",
       " ('Microsoft', 'Ability'),\n",
       " ('communication', 'years'),\n",
       " ('Ability', 'software'),\n",
       " ('Armenian', 'interpersonal'),\n",
       " ('with', 'computer'),\n",
       " ('and', 'experience'),\n",
       " ('field', 'written'),\n",
       " ('Strong', 'Office'),\n",
       " ('written', 'team'),\n",
       " ('skills', 'and'),\n",
       " ('team', 'plus'),\n",
       " ('Excellent', 'with'),\n",
       " ('including', 'skills'),\n",
       " ('skills', 'including'),\n",
       " ('applications', 'languages'),\n",
       " ('skills', 'English'),\n",
       " ('relevant', 'ability'),\n",
       " ('least', 'Excellent'),\n",
       " ('field', 'team'),\n",
       " ('communication', 'Fluency'),\n",
       " ('organizational', 'degree'),\n",
       " ('languages', 'Experience'),\n",
       " ('management', 'Good'),\n",
       " ('with', 'Russian'),\n",
       " ('years', 'the'),\n",
       " ('field', 'ability'),\n",
       " ('University', 'ability'),\n",
       " ('Economics', 'degree'),\n",
       " ('Excel', 'with'),\n",
       " ('English', 'skills'),\n",
       " ('plus', 'the'),\n",
       " ('language', 'management'),\n",
       " ('including', 'and'),\n",
       " ('degree', 'Higher'),\n",
       " ('education', 'Ability'),\n",
       " ('work', 'Good'),\n",
       " ('relevant', 'degree'),\n",
       " ('work', 'working'),\n",
       " ('Experience', 'work'),\n",
       " ('work', 'education'),\n",
       " ('experience', 'year'),\n",
       " ('Russian', 'Knowledge'),\n",
       " ('ability', 'degree'),\n",
       " ('team', 'years'),\n",
       " ('Ability', 'University'),\n",
       " ('Strong', 'knowledge'),\n",
       " ('and', 'English'),\n",
       " ('English', 'experience'),\n",
       " ('languages', 'work'),\n",
       " ('Russian', 'with'),\n",
       " ('languages', 'language'),\n",
       " ('experience', 'languages'),\n",
       " ('knowledge', 'and'),\n",
       " ('with', 'Knowledge'),\n",
       " ('least', 'with'),\n",
       " ('and', 'skills'),\n",
       " ('skills', 'experience')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures() #Creating bigram_measures object\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(all_job_postings_words)\n",
    "                                                #Creating bigram_finder object using all word list obtained from previous step\n",
    "bigram_finder.apply_freq_filter(10) #Filter out the bigrams which occur less than 10 times in the enntire vocabulary\n",
    "bigram_finder.apply_word_filter(lambda w: len(w) < 3)#Ignoring bigrams less than length 3\n",
    "top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200) # making list of top 200 bigrams\n",
    "top_200_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Tokenized Dictionary and All Vocab List with Bigrams\n",
    "In this step the function MWETokenizer() merges each the top_200_bigrams into unigrams with a separator '\\_'. The  mwetokenizer is an iterative object which is further used to create a dictionary named all_job_postings_dict_tokenized_with_bigram that consists of all unigram version of bigram tokens with the separate words of each bigram being removed. This is done with the help of mwetokenizer.tokenize() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwetokenizer = MWETokenizer(top_200_bigrams) #Merging bigrams with '_' through multiword tokenizer\n",
    "all_job_postings_dict_tokenized_with_bigram =  dict((pid, mwetokenizer.tokenize(patent)) for pid,patent in all_job_postings_dict_tokenized.items())\n",
    "                                    #Inclusion of biagrams into the tokenized dictionary, ingnoring their individual occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming on Word Tokens at each Resume Level\n",
    "This step finally does the stemming on all_job_postings_dict_tokenized_with_bigram. After this step, all_job_postings_dict_tokenized_with_bigram will consist of document id as the key and stemmed tokens of each job responsibility content. The input for stemmer program is each word from the list(dictionary value) and the output dictionary from this step is **all_job_postings_dict_tokenized_with_bigram**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()  #Creating stemmer object\n",
    "final_tokens =[]\n",
    "### Taking each list of word tokens with unigram and bigrams for individual documents\n",
    "for keys, values in all_job_postings_dict_tokenized_with_bigram.items():\n",
    "    final_tokens = final_tokens + [stemmer.stem(w) for w in values ] #Stemming the token list\n",
    "    all_job_postings_dict_tokenized_with_bigram[keys] = final_tokens #Replacing the older token list with updated one\n",
    "    final_tokens = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Separate Lists of Job Ids and Job Responsibilities\n",
    "In order to do this, first, two separate lists each for job id and job content is initialized. Then the all_job_postings_dict_tokenized_with_bigram is traversed to populate the job ids into id list and job responsibilities in job content list. Join() method is used to convert the list of string into a string of pace separator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_list = []    #Initializing two lists\n",
    "job_content_list = []\n",
    "#Iterating for each tokenized list after garbage tokens ramoval\n",
    "for pid, tokens in all_job_postings_dict_tokenized_with_bigram.items():\n",
    "    job_id_list.append(pid)  #Appending job ids to job is list\n",
    "    text = ' '.join(tokens)   #Joining back tokens into strings\n",
    "    job_content_list.append(text)  #Appending job responsibilities into job content list\n",
    "    text = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Creating Separate Lists of Resume Ids and Resume Details after Stopword Removal, Context Dependent and Rare Token Roval and Stemming\n",
    "The following steps need to be performed in order to do the above \n",
    "## Reading Input Files:\n",
    "### Reading Resume numbers from 'resume_dataset.txt'\n",
    "In this step, the 'resume_dataset.txt', provided in the datasets, is searched for 29453232 to get the following list of resume number. Below is the format of the document:\n",
    "\n",
    "29453232:\\[0 12 23 ------]\n",
    "\n",
    "So in order to extract the document ids, first the correponding number(29453232 in this case) is found in the document list and then the following string enclosed by a '[]' is extracted through the regular expression: ('29453232:\\[(.*?)\\]') Once the string is found, all the '\\n' are replaced with a ' ' and the replaced string is split on ' ' to get the document numbers. The whole list is converted into a set and then again into a list in order to remove the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "readfile = open('resume_dataset.txt').read() #Reading the file\n",
    "readfile_29453232_data = re.findall('29453232:\\[(.*?)\\]',readfile,re.S)[0] #Finding the resume numbers for '29453232'\n",
    "readfile_29453232_data  = readfile_29453232_data.replace('\\n', ' ') #Replacing all '\\n' with ' '\n",
    "readfile_29453232_list = readfile_29453232_data.split(' ') #Splitting the whole string into the list of resume numbers\n",
    "readfile_29453232_list = list(set([ items for items in readfile_29453232_list if items != '' ]))\n",
    "                        #Removing the null items, removing duplicate resume numbers and finally putting them into final list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Contents of the Resumes\n",
    "After the resume numbers are extracted, the proper file names are built with the following format:\n",
    "**resume(123).txt**\n",
    "For this, the resume number is sandwiched inside the string **'resume()'**. When resume id is found zero, then 1 is passed within the string. Once the resume ids are generated, resumes are read and entered into into the list **'all_resume_list'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_resume_list = []\n",
    "#Building resume names for each resume numbers obtained from previous step\n",
    "for items in readfile_29453232_list:  \n",
    "    if items == '0':  #If resume number is '0' then the resume name will be resume_(1).txt\n",
    "        build_resume_str = 'resume_(' + '1' + ').txt'\n",
    "    else: #For others the resume name will be resume_(<Resume No>).txt\n",
    "        build_resume_str = 'resume_(' + items + ').txt'\n",
    "    read_resume = open(build_resume_str,'r',encoding='UTF-8') #Opening and read each resume file in read mode\n",
    "    all_resume_list.append((items, read_resume.read()))\n",
    "    read_resume.close() #Closing resume file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenizing, Word Tokenizing, Stop Word Removal and Token of Length Less Than Three Removal\n",
    "In this step many accomplishments such as Sentence Tokenization, Word Tokenization, Stop Word Removal and removal of Token length less than three take place. \n",
    "Firstly, the list **all_resume_list** is iterated. For each resume in that list the following activities are performed:\n",
    "> - Each resume is tokenized into sentences through sentence tokenizer.\n",
    "> - Now for each sentence in the sentence list the below operations take place:\n",
    "    > - Each sentence is further tokenized into word tokens with the use of regular expression r\"\\w+(?:[-.]\\w+)?\"\n",
    "    > - The first token of each sentence is lowercased\n",
    "    > - The tokens with length less than three are removed\n",
    "    > - The word tokens are joined back to a complete sentence\n",
    "    > - Each parsed sentence are again joined back to the resume content\n",
    "> - The Document Number and Parsed Resume ontent are placed into a dictionary name 'all_resume_dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_profile = []\n",
    "#Iterating each resume from the resume list\n",
    "for each_resume_tuple in all_resume_list:\n",
    "    item_index = all_resume_list.index(each_resume_tuple) #Taking down the index number\n",
    "    sentences = nltk.data.load('tokenizers/punkt/english.pickle') #Sentence Tokenizing\n",
    "    sentences_list = sentences.tokenize(each_resume_tuple[1].strip()) #Stripping spaces from the ends\n",
    "    each_sentence_wo_sw_lc = ''\n",
    "    all_sentence_wo_sw_lc = ''\n",
    "    #Iterating each sentence from the sentence list\n",
    "    for each_sentence in sentences_list:\n",
    "        tokenizer = RegexpTokenizer(r\"\\w+(?:[-.]\\w+)?\")  #Word Tokenization is done on this Regular Expression\n",
    "        unigram_tokens = tokenizer.tokenize(each_sentence) #Unigram tokens are obtained\n",
    "        if len(unigram_tokens) > 0:  #Ignoring the null sentences\n",
    "            unigram_tokens[0] = unigram_tokens[0].lower()  #Lowercasing the 1st word of each sentence\n",
    "            \n",
    "            #Removing the stopwords\n",
    "            unique_unigram_tokens_wo_sw = [ items for items in unigram_tokens if items.lower() not in stop_word_list ]\n",
    "            #Removing tokens of length less than 3\n",
    "            unique_unigram_tokens_wo_sw = [ items for items in unique_unigram_tokens_wo_sw if len(items) >= 3 ]\n",
    "            #Joining back the word tokens into complete sentences\n",
    "            each_sentence_wo_sw_lc = ' '.join(unique_unigram_tokens_wo_sw)\n",
    "            all_sentence_wo_sw_lc += each_sentence_wo_sw_lc + ' '  #Joining back all the sentences\n",
    "            \n",
    "    all_resume_list[item_index] = (each_resume_tuple[0], all_sentence_wo_sw_lc) \n",
    "                            #Putting resume number and resume contents into each tuple\n",
    "all_resume_dict = dict(all_resume_list)  #Converting the tuple into dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tokenized Dictionary and All Vocab List\n",
    "In this step two different objects are formed:\n",
    "> - all_resume_dict_tokenized dictinary that consists of each document id as key and list of unigram tokens for each document as value. This dictionary is created by invoking a method named tokenizePatent() which actually generates unigram word tokens from the string of each resume content.\n",
    "> - From all_resume_dict_tokenized dictionary, the list of all vocabularies is created by chain.from_iterable() method which takes the list of lists as input and returns a single list of all items across this list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizePatent(pid):\n",
    "    \"\"\"\n",
    "        the tokenization function is used to tokenize each patent.\n",
    "        The one argument is patent_id.\n",
    "        First, normalize the case.\n",
    "        Then, use the regular expression tokenizer to tokenize the patent with the specified id\n",
    "    \"\"\"\n",
    "    each_dict_item = all_resume_dict[pid]\n",
    "    each_dict_item_tokenized = tokenizer.tokenize(each_dict_item)\n",
    "    return (pid, each_dict_item_tokenized) # return a tupel of patent_id and a list of tokens\n",
    "\n",
    "#Getting the tokenized version of the dictionary where each word token is separated\n",
    "all_resume_dict_tokenized = dict(tokenizePatent(pid) for pid in all_resume_dict.keys())\n",
    "#Getting the whole word lists\n",
    "all_resume_words = list(chain.from_iterable(all_resume_dict_tokenized.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Top 200 Bigrams\n",
    "A bigram is an asociation of two meaningful words. The below code creates top 200 meaningful bigrams in form of a list of tuples with the help of the full vacab list all_resume_words, created in the previous step. This program also set frequency filter at the level of 10 and also filters out those bigrams with length less than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures() #Creating bigram_measures object\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(all_resume_words)\n",
    "                                    #Creating bigram_finder object using all word list obtained from previous step\n",
    "bigram_finder.apply_freq_filter(10) #Filter out the bigrams which occur less than 10 times in the enntire vocabulary\n",
    "bigram_finder.apply_word_filter(lambda w: len(w) < 3) #Ignoring bigrams less than length 3\n",
    "top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200) # making list of top 200 bigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Tokenized Dictionary and All Vocab List with Bigrams\n",
    "In this step the function MWETokenizer() merges each the top_200_bigrams into unigrams with a separator '\\_'. The  mwetokenizer is an iterative object which is further used to create a dictionary named all_resume_dict_with_bigram that consists of all unigram version of bigram tokens with the separate words of each bigram being removed. This is done with the help of mwetokenizer.tokenize() function. Finally, the all_resume_vocabs list is created which also consists of the entire resume vocabulary with unigram version of bigrams included and separate words for each bigram removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwetokenizer = MWETokenizer(top_200_bigrams)  #Merging bigrams with '_' through multiword tokenizer\n",
    "all_resume_dict_with_bigram =  dict((pid, mwetokenizer.tokenize(patent)) for pid,patent in all_resume_dict_tokenized.items())\n",
    "                    #Inclusion of biagrams into the tokenized dictionary, ingnoring their individual occurrences\n",
    "all_resume_vocabs = list(chain.from_iterable(all_resume_dict_with_bigram.values()))\n",
    "                    #Inclusion of biagrams, ingnoring their individual occurrences\n",
    "all_resume_vocabs = list(set(all_resume_vocabs)) #Removing duplicate words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification of Context Dependent Tokens and Rare Tokens\n",
    "The word tokens that appear in 98% or more documents are called Context Dependent Tokens, whereas word tokens that appear in 2% or less documents are called Rare Tokens. Removal of these tokens is necessary in any text procesing technique. The below code identifies words of these two categories and place them into appropriate lists. In order to identify such words a dictionary of words with corresponding frequencies is created. Now each word of this dictionary(in thi case, all_words_document_freq) is travarsed, and depending on its frequency value in respect to 98% or 2%, it is placed either in context_dependent_token_list or in rare_token_list. Now these two lists are merged to get the complete list of the tokens to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_dependent_token_list = []\n",
    "rare_token_list = []\n",
    "\n",
    "all_words_document_freq = {}\n",
    "#Iterating the dictionary with unigram and bigram eord tokens\n",
    "for keys,lists in all_resume_dict_with_bigram.items():\n",
    "    for items in set(lists): #Taking each unique unigrams and bigrams\n",
    "        if all_words_document_freq.get(items) == None: #If the word is not yet discovered then making its freq 1\n",
    "            all_words_document_freq[items] = 1\n",
    "        else:                   #If the word is discovered then increasing its freq by 1\n",
    "            all_words_document_freq[items] += 1\n",
    "\n",
    "for keys, values in all_words_document_freq.items(): #Traversing this dictionary containing all vocabs and their document frequencies\n",
    "    all_words_document_freq[keys] = (values/len(all_resume_dict_with_bigram)) * 100\n",
    "                                    #Calculating the document frequency of each token\n",
    "    if all_words_document_freq[keys] >= 98: #If the document frequency is more than 98%, placing it into context dependent list\n",
    "        context_dependent_token_list.append(keys)\n",
    "    elif all_words_document_freq[keys] <= 2: #If the document frequency is less than 2%, placing it into rare token list\n",
    "        rare_token_list.append(keys)\n",
    "#Mergng context_dependent_token_list and rare_token_list into one list total_list\n",
    "if len(context_dependent_token_list) > 0:\n",
    "    total_list = context_dependent_token_list + rare_token_list\n",
    "else:\n",
    "    total_list = rare_token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Context Dependent and Rare Tokens from each Resume Level\n",
    "This step removes the Context Dependent and Rare Tokens from the all_resume_dict_with_bigram dictionary, the dictionary with key as Resume Number and value as list of Unigrams and Bigrams in each Resume, making it ready for Stemming at each resume level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the context dependent and raretokens from the list of word tokens for each resume\n",
    "for key, value in all_resume_dict_with_bigram.items():\n",
    "    all_resume_dict_with_bigram[key] = [ items for items in value if items not in total_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming on Word Tokens at each Resume Level\n",
    "This step finally does the stemming on all_resume_dict_with_bigram. After this step, all_resume_dict_with_bigram will consist of document id as the key and stemmed tokens of each resume content. The input for stemmer program is each word from the list(dictionary value) and the output dictionary from this step is **all_resume_dict_with_bigram**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()  #Creating stemmer object\n",
    "final_tokens =[]\n",
    "### Taking each list of word tokens with unigram and bigrams for individual documents\n",
    "for keys, values in all_resume_dict_with_bigram.items():\n",
    "    final_tokens = final_tokens + [stemmer.stem(w) for w in values ] #Stemming the token list\n",
    "    all_resume_dict_with_bigram[keys] = final_tokens #Replacing the older token list with updated one\n",
    "    final_tokens = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Separate Lists of Resume Ids and Resume Details\n",
    "In order to do this, first, two separate lists each for resume id and resume details is initialized. Then the all_resume_dict_with_bigram is traversed to populate the resume ids into id list and resume details in job content list. Join() method is used to convert the list of string into a string of pace separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_id_list = []  #Initializing two lists\n",
    "reume_content_list = []\n",
    "#Iterating for each tokenized list after garbage tokens ramoval\n",
    "for pid, tokens in all_resume_dict_with_bigram.items(): \n",
    "    resume_id_list.append(pid) #Appending job ids to job is list\n",
    "    text = ' '.join(tokens)    #Joining back tokens into strings\n",
    "    reume_content_list.append(text)  #Appending job responsibilities into job content list\n",
    "    text = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Building the Recommender System and Place the Whole Recommended Content Output File \n",
    "In the below activity the, a final output file is generated with top 10 resumes against each job postings. Here, fistly job_content_list, generated in previous step is 1st traversed. For each job content, the following tasks are done: \n",
    "> - A temporary list temp_reume_content_list is created and whole resume content is copied\n",
    "> - The Job Content is then appended at first of this temp_reume_content_list to compare all resumes with that particular job content.\n",
    "> - The tfidf_vectorizer vector object is created and all resumes are matched with the job content through \n",
    "cosine_similarity() function\n",
    "> - The cosine similarity of each resume and the resume numbers are combines and sorted on the similarity values.\n",
    "> - The top ten resume numbers are selected depending on the higher similarity values\n",
    "> - The top 10 resumes are placed in the output file in the given format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reume_content_list\n",
    "count = 1\n",
    "job_recommend_dict = {}  #Defining recommender dictionary\n",
    "for items in job_content_list:   #Iterating on each job content\n",
    "    index = job_content_list.index(items)  #Taking down the list index\n",
    "    temp_reume_content_list = []\n",
    "    \n",
    "    temp_reume_content_list = [item for item in reume_content_list]  #Copying list data into temporary list\n",
    "    temp_reume_content_list.insert(0, items) #Insert the job details into the 1st position of the list\n",
    "    tfidf_vectorizer = TfidfVectorizer() #Creating tfidf_vectorizer object\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(temp_reume_content_list) #Creating tfidf_matrix\n",
    "    cosine_similarity_list = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix).tolist()\n",
    "                    #Generating the Cosine Similarity Matrix\n",
    "    cosine_similarity_list = cosine_similarity_list[0] #Converting into 1st level of list\n",
    "    del(cosine_similarity_list[0])  #Delete the job content\n",
    "    resume_recommend_zip = zip(cosine_similarity_list, resume_id_list) #Creating the list of tuple on Cosine similatity\n",
    "    resume_recommend_list = list(resume_recommend_zip)\n",
    "    resume_recommend_list.sort(key=lambda tup: tup[0], reverse=True) #Sorting the list on reverse\n",
    "    resume_recommend_dict = dict(resume_recommend_list) #Creating the recommendation dictionary\n",
    "    resume_recommend_list_of_10 = list(islice(resume_recommend_dict.items(), 10)) #Giving Top 10 recommendation\n",
    "    resume_recommend_list_of_10 = [items[1] for items in resume_recommend_list_of_10]\n",
    "    job_recommend_dict[job_id_list[index]] = resume_recommend_list_of_10 #Creating Final dictionary\n",
    "\n",
    "#Writing the output file Recommended_Resume.txt\n",
    "bonus_29453232 = open('Recommended_Resume.txt', 'w')\n",
    "for keys, values in job_recommend_dict.items():\n",
    "    file_write_str = ''\n",
    "    file_write_str = keys + ': '\n",
    "    count = 1\n",
    "    for items in values:\n",
    "        if count != 10:\n",
    "            file_write_str += items + ', '\n",
    "        else:\n",
    "            file_write_str += items + '\\n'\n",
    "        count += 1\n",
    "    file_write_str = file_write_str.strip(',')\n",
    "    bonus_29453232.write(file_write_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
